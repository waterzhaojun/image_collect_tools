{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8-yl-s-WKMG"
   },
   "source": [
    "# Object Detection Demo\n",
    "Welcome to the object detection inference walkthrough!  This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFSqkTCdWKMI"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [],
   "source": [
    "from distutils.version import StrictVersion\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "research_path = '/Users/Melody/OneDrive/MLprojects/models-master/research'\n",
    "sys.path.append(research_path)\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):\n",
    "  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfn_tRFOWKMO"
   },
   "source": [
    "# Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_sEBLpVWKMQ"
   },
   "source": [
    "## Variables\n",
    "\n",
    "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file.  \n",
    "\n",
    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VyPz_t8WWKMQ"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = os.path.join(research_path, 'object_detection/rodent_dataset/model')\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join(research_path, 'object_detection/rodent_dataset/data/labelmap.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBcB9QHLWKMU"
   },
   "source": [
    "## Load a (frozen) Tensorflow model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KezjCRVvWKMV"
   },
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MVVTcLWKMW"
   },
   "source": [
    "## Loading label map\n",
    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hDbpHkiWWKMX"
   },
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EFsoUHvbWKMZ"
   },
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aSlYc3JkWKMa"
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0_1AGhrWKMc"
   },
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jG-zn5ykWKMd"
   },
   "outputs": [],
   "source": [
    "# This is the place to set your image folder need to be cropped.\n",
    "PATH_TO_TEST_IMAGES_DIR = '/Users/Melody/OneDrive/lab_data/data/video/behavior/rat_side_20180918_2'\n",
    "TEST_IMAGE_PATHS = os.listdir(PATH_TO_TEST_IMAGES_DIR)\n",
    "TEST_IMAGE_PATHS = [os.path.join(PATH_TO_TEST_IMAGES_DIR,x) for x in TEST_IMAGE_PATHS if x[-4:]=='.jpg']\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "#IMAGE_SIZE = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "92BHxzcNWKMf"
   },
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "              tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "      # Run inference\n",
    "      output_dict = sess.run(tensor_dict,\n",
    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.uint8)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_object_image(image_np_array, savepath, output_dict, normalized = True, save_cropped_img = True, \n",
    "                      show_cropped_img = False, img_size = (12, 8), score_threshold = 0.5):\n",
    "    \n",
    "    # get the boxes\n",
    "    idxes = output_dict['detection_scores'] > score_threshold\n",
    "    boxes = output_dict['detection_boxes'][idxes]\n",
    "    \n",
    "    # box is a list with size 4\n",
    "    height, width,__ = image_np_array.shape\n",
    "    \n",
    "    for box in boxes:\n",
    "        newbox = np.copy(box)\n",
    "        if normalized:\n",
    "            newbox = (newbox * [height, width, height, width]).astype(int)\n",
    "        \n",
    "        print(newbox)\n",
    "\n",
    "        image_crop = image_np_array[newbox[0]:newbox[2], newbox[1]:newbox[3], :]\n",
    "\n",
    "        if save_cropped_img:\n",
    "            im = Image.fromarray(image_crop)\n",
    "            im.save(savepath)\n",
    "\n",
    "        if show_cropped_img:\n",
    "            plt.figure(figsize=img_size)\n",
    "            plt.imshow(image_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 41 819 205 959]\n",
      "[161 811 491 946]\n",
      "[ 74 791 271 942]\n",
      "[379 761 718 943]\n",
      "[ 30 798 390 945]\n",
      "[  74 1082  266 1231]\n",
      "[ 50 815 193 939]\n",
      "[ 39 789 207 943]\n",
      "[ 37 811 199 939]\n",
      "[  73 1083  262 1237]\n",
      "[375 805 646 932]\n",
      "[ 40 817 200 961]\n",
      "[ 50 811 187 967]\n",
      "[ 59 793 210 960]\n",
      "[ 40 820 198 953]\n",
      "[116 753 297 939]\n",
      "[ 33 781 229 932]\n",
      "[ 63 776 243 943]\n",
      "[ 67 801 207 935]\n",
      "[ 41 815 196 953]\n",
      "[ 62 822 266 955]\n",
      "[441 770 671 933]\n",
      "[ 39 822 199 956]\n",
      "[ 50 645 254 945]\n",
      "[  77 1086  263 1241]\n",
      "[ 44 814 200 973]\n",
      "[  71 1094  264 1245]\n",
      "[ 72 803 261 938]\n",
      "[ 57 808 256 948]\n",
      "[  73 1094  262 1248]\n",
      "[ 38 819 199 958]\n",
      "[  73 1091  262 1238]\n",
      "[ 62 808 188 945]\n",
      "[  71 1091  266 1237]\n",
      "[ 58 803 244 951]\n",
      "[  70 1089  267 1234]\n",
      "[ 91 828 456 945]\n",
      "[  67 1088  265 1237]\n",
      "[ 38 818 196 953]\n",
      "[ 44 816 200 961]\n",
      "[ 57 749 190 941]\n",
      "[133 816 512 945]\n",
      "[ 42 805 242 943]\n",
      "[132 810 226 923]\n",
      "[ 43 817 202 980]\n",
      "[448 692 562 883]\n",
      "[  70 1091  268 1235]\n",
      "[ 72 806 232 954]\n",
      "[ 63 824 265 950]\n",
      "[ 57 801 192 934]\n",
      "[361 827 578 926]\n",
      "[ 47 819 201 965]\n",
      "[ 49 803 192 937]\n",
      "[  69 1091  261 1258]\n",
      "[374 835 613 936]\n",
      "[ 60 809 289 942]\n",
      "[ 70 806 219 932]\n",
      "[  71 1085  261 1239]\n",
      "[ 36 802 311 947]\n",
      "[ 43 808 278 948]\n",
      "[ 36 817 197 948]\n",
      "[  74 1093  267 1246]\n",
      "[ 66 804 284 957]\n",
      "[ 50 818 202 971]\n",
      "[ 57 783 229 941]\n",
      "[  71 1090  260 1243]\n",
      "[ 85 802 282 964]\n",
      "[ 52 808 206 944]\n",
      "[ 55 819 201 972]\n",
      "[415 812 628 926]\n",
      "[  72 1088  261 1241]\n",
      "[ 39 811 289 952]\n",
      "[340 637 486 774]\n",
      "[  74 1085  265 1234]\n",
      "[ 71 797 196 950]\n",
      "[ 66 802 224 935]\n",
      "[161 821 238 944]\n",
      "[  74 1093  264 1253]\n",
      "[ 54 807 268 958]\n",
      "[ 61 822 359 956]\n",
      "[  72 1087  265 1243]\n",
      "[ 64 812 287 957]\n",
      "[  73 1088  266 1236]\n",
      "[ 78 823 249 939]\n",
      "[ 56 811 198 951]\n",
      "[  68 1091  263 1242]\n",
      "[191 774 350 921]\n",
      "[ 60 809 273 964]\n",
      "[  75 1092  262 1236]\n",
      "[ 75 802 268 955]\n",
      "[  72 1090  265 1233]\n",
      "[ 87 795 218 947]\n",
      "[ 64 812 268 949]\n",
      "[ 74 813 202 935]\n",
      "[ 60 796 236 953]\n",
      "[  68 1088  263 1245]\n",
      "[ 82 802 258 948]\n",
      "[ 83 814 360 951]\n",
      "[  67 1089  265 1242]\n",
      "[109 824 348 931]\n",
      "[ 45 801 212 961]\n",
      "[438 590 595 804]\n",
      "[154 769 282 919]\n",
      "[ 41 797 196 936]\n",
      "[  71 1084  262 1232]\n",
      "[ 95 800 230 935]\n",
      "[476 645 600 848]\n",
      "[ 60 817 202 952]\n",
      "[  66 1094  264 1244]\n",
      "[ 54 802 283 947]\n",
      "[  73 1091  268 1238]\n",
      "[ 47 817 202 969]\n",
      "[  69 1089  268 1234]\n",
      "[ 73 809 247 953]\n",
      "[  75 1088  264 1234]\n",
      "[ 76 820 301 945]\n",
      "[ 68 804 188 946]\n",
      "[  74 1086  267 1233]\n",
      "[ 50 791 220 946]\n",
      "[171 834 422 924]\n",
      "[ 72 822 207 948]\n",
      "[  72 1085  263 1237]\n",
      "[ 86 645 223 932]\n",
      "[  73 1085  262 1235]\n",
      "[ 39 819 196 954]\n",
      "[ 56 798 191 941]\n",
      "[ 50 818 201 972]\n",
      "[ 38 818 195 956]\n",
      "[ 70 784 241 945]\n",
      "[ 58 793 190 929]\n",
      "[ 81 808 356 946]\n",
      "[263 745 610 941]\n",
      "[ 70 798 281 950]\n",
      "[  73 1086  265 1237]\n",
      "[ 46 798 224 952]\n",
      "[  69 1088  264 1235]\n"
     ]
    }
   ],
   "source": [
    "# This part is tested good to produce cropped data.\n",
    "\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  image = Image.open(image_path)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "  # image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "  # crop the image_np and save to local\n",
    "  folderpath = os.path.dirname(image_path)\n",
    "  filename = os.path.basename(image_path)\n",
    "  savepath = os.path.join(folderpath, 'crop_'+filename)\n",
    "  crop_object_image(image_np, savepath, output_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
